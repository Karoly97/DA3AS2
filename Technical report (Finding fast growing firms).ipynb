{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5357d84",
   "metadata": {},
   "source": [
    "# <center><font color='magenta'>**Assignment 2 for DA3**</font></center>\n",
    "### <center>Central European University 2025</center>\n",
    "## <center> Technical report (Finding fast growing firms) </center>\n",
    "\n",
    "#### <center> Created by: Gréta Zsikla & Károly Takács</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255ac536",
   "metadata": {},
   "source": [
    "# Summary Report: Predicting Fast-Growing Firms 2025\n",
    "\n",
    "## 1. Overview\n",
    "\n",
    "This technical report details the methodology, decisions, and results of predicting fast-growing firms using the `bisnode-firms` dataset (2010-2015), supporting investment strategies for 2025. The target, `f_growth`, is a binary variable (sales growth ≥20%, 2012-2013), and we evaluate multiple models—logistic regression (M1, M2, LASSO) and Random Forest (RF)—across Task 1 (model selection) and Task 2 (industry comparison: Manufacturing vs. Services). RF emerges as the best model, with all code available at [GitHub: DA3AS2](https://github.com/Karoly97/DA3AS2/tree/main).\n",
    "\n",
    "\n",
    "## 2. Data Preparation\n",
    "\n",
    "### 2.1 Data Source and Initial Processing\n",
    "- **Source**: `cs_bisnode_panel.csv` (287,829 rows), loaded from [OSF](https://osf.io/mbu3d).\n",
    "- **Decision**: Filter to 2012-2013 (56,943 rows), focusing on 2013 for prediction (14,689 firms post-cleaning). Rationale: One-year growth balances data availability and relevance, avoiding sparsity in 2014-2015.\n",
    "\n",
    "\n",
    "<details><summary>load data</summary>\n",
    "  ```python\n",
    "  df = pd.read_csv(\"https://osf.io/mbu3d/download\")\n",
    "  clean = df[df['year'].isin([2012, 2013])]\n",
    "  ```\n",
    "</details>\n",
    "\n",
    "### 2.2 Cleaning and Feature Engineering\n",
    "\n",
    "#### Cleaning Decisions:\n",
    "- Drop high-NA columns (e.g., `COGS`, 94% missing) to reduce noise.\n",
    "- Cap negative assets (e.g., `curr_assets`) at 0, flagging issues (`flag_asset_problem`).\n",
    "- Impute `ceo_age` with mean (50.5) for continuity.\n",
    "\n",
    "#### Feature Engineering:\n",
    "- **Ratios**: `personnel_exp_pl = personnel_exp/sales`, normalized by total assets or sales.\n",
    "- **Quadratic terms**: `profit_loss_year_pl_quad` to capture non-linearity.\n",
    "- **Remove leakage**: Drop `sales12`, `growth` from predictors to ensure integrity.\n",
    "\n",
    "<details><summary>Feature Engineering</summary>\n",
    "```python\n",
    "clean[\"total_assets_bs\"] = clean[[\"intang_assets\", \"curr_assets\", \"fixed_assets\"]].sum(axis=1)\n",
    "clean[\"personnel_exp_pl\"] = clean[\"personnel_exp\"] / clean[\"sales\"]\n",
    "clean[\"ceo_age\"] = np.where(clean[\"ceo_age\"].isna(), clean[\"ceo_age\"].mean(), clean[\"ceo_age\"])\n",
    "clean = clean.drop(columns=[\"sales12\", \"growth\", \"growth2\"])\n",
    "```\n",
    "</details>\n",
    "\n",
    "**Rationale**: Ratios enhance interpretability, flags add categorical insights, and leakage removal ensures predictions rely on 2012 data only.\n",
    "\n",
    "### 2.3 Target Definition\n",
    "\n",
    "**Decision**: `f_growth = 1` if `(sales_2013 - sales_2012) / sales_2012 ≥ 20%` (3,810 positive, 26%).\n",
    "\n",
    "**Rationale**: 20% exceeds typical growth (5-10%), aligning with corporate finance’s focus on high-return firms. Two-year growth rejected due to data gaps.\n",
    "\n",
    "<details><summary>Targeting</summary>\n",
    "```python\n",
    "pivoted = clean.pivot(index='comp_id', columns='year', values='sales')\n",
    "clean[\"f_growth\"] = np.where(((pivoted[2013] - pivoted[2012]) / pivoted[2012]) * 100 >= 20, 1, 0)\n",
    "```\n",
    "</details>\n",
    "---\n",
    "\n",
    "## 3. Task 1: Model Development and Selection\n",
    "\n",
    "### 3.1 Model Specifications\n",
    "\n",
    "- **M1 (Logit with Splines)**: 15 variables, e.g., `lspline(amort, [125000])` for non-linear effects.\n",
    "- **M2 (Simple Logit)**: 19 variables, linear terms, e.g., `age`, `foreign`.\n",
    "- **LASSO Logit**: 23 variables, L1 regularization, 20 non-zero coefficients post-tuning.\n",
    "- **Random Forest (RF)**: 18 variables, tuned (`max_features=5, min_samples_split=90`).\n",
    "\n",
    "**Decision**: Exceed requirement (3 models) with 4 to explore diverse approaches.\n",
    "\n",
    "\n",
    "<details><summary>RF Tuning</summary>\n",
    "```\n",
    "grid = {\"max_features\": range(1, 6), \"min_samples_split\": range(80, 100, 5)}\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state=50)\n",
    "rf_grid = GridSearchCV(rf, grid, cv=5, scoring=\"roc_auc\")\n",
    "```\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 3.2 Evaluation Metrics\n",
    "\n",
    "- **Metrics**: CV AUC, RMSE, expected loss (`FP=1`, `FN=10`).\n",
    "- **Decision**: Use 5-fold CV for robustness; loss prioritizes recall (`FN` costlier).\n",
    "\n",
    "#### Results (Table 1):\n",
    "\n",
    "| Model  | Features | CV AUC | CV RMSE | Expected Loss (Threshold) |\n",
    "|--------|---------|--------|---------|---------------------------|\n",
    "| M1     | 26      | 0.637  | 0.436   | 0.68 (0.06) * |\n",
    "| M2     | 20      | 0.629  | 0.437   | 0.69 (0.06) * |\n",
    "| LASSO  | 20      | 0.632  | 0.436   | 0.68 (0.06) * |\n",
    "| RF     | 18      | 0.660  | 0.431   | 0.68 (0.06) |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 Model Selection\n",
    "\n",
    "- **Decision**: Choose **RF** for best CV AUC (0.660) and RMSE (0.431).  \n",
    "- **Rationale**: RF captures non-linear patterns (e.g., `personnel_exp_pl`) better than logits.  \n",
    "- Bug in original selection logic (M1) was corrected during re-analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 Holdout Performance\n",
    "\n",
    "**RF Results**  \n",
    "- AUC = 0.65  \n",
    "- RMSE = 0.434  \n",
    "- Expected loss ≈ 0.68 (threshold = 0.06)\n",
    "\n",
    "<details><summary>Confusion Matrix</summary>\n",
    "\n",
    "```\n",
    "TN = 555\n",
    "FP = 1484\n",
    "FN = 0\n",
    "TP = 723\n",
    "\n",
    "Recall = 1.0, Precision = 0.33\n",
    "```\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Task 2: Industry-Specific Analysis\n",
    "\n",
    "## 4.1 Industry Split\n",
    "\n",
    "- **Decision**: Split by NACE:  \n",
    "  - **Manufacturing** (1000–3400), 4,861 firms  \n",
    "  - **Services** (≥4500), 9,828 firms  \n",
    "\n",
    "<details><summary>Code Snippet</summary>\n",
    "\n",
    "```python\n",
    "def classify_industry(nace):\n",
    "    return 'manufacturing' if 1000 <= nace < 3400 else 'services' if nace >= 4500 else 'other'\n",
    "\n",
    "clean['industry'] = clean['nace_main'].apply(classify_industry)\n",
    "```\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 4.2 RF Application\n",
    "\n",
    "- **Decision**: Apply RF with the same loss function (FP=1, FN=10) and threshold optimization.\n",
    "\n",
    "**Results (Table 2)**\n",
    "\n",
    "| Industry       | Threshold | Expected Loss | AUC   | Precision | Recall | F1 Score |\n",
    "|----------------|-----------|--------------:|------:|----------:|-------:|---------:|\n",
    "| **Manufacturing** | 0.060   | 0.680        | 0.691 | 0.301     | 0.998  | 0.462    |\n",
    "| **Services**      | 0.060   | 0.682        | 0.683 | 0.265     | 0.998  | 0.418    |\n",
    "\n",
    "---\n",
    "\n",
    "## 4.3 Feature Importance\n",
    "\n",
    "- **Manufacturing**: `personnel_exp_pl (0.040)`, `profit_loss_year (0.032)`.  \n",
    "- **Services**: `profit_loss_year (0.031)`, `sales (0.030)`.\n",
    "\n",
    "<details><summary>Graph (Placeholder)</summary>\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "feats = pd.Series(rf.feature_importances_, index=X.columns).nlargest(5)\n",
    "feats.plot(kind='barh', title='Top 5 Features (Manufacturing)')\n",
    "plt.savefig('feature_importance.png')\n",
    "```\n",
    "</details>\n",
    "\n",
    "**Figure 1**: Feature Importance (Manufacturing)\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Discussion\n",
    "\n",
    "## 5.1 Data Decisions\n",
    "\n",
    "- **20% Threshold**: Balances sensitivity and sample size; a 50% threshold would reduce positives excessively.  \n",
    "- **Leakage Removal**: Essential for validity (e.g., dropping `sales12`); note EDA with LOWESS was limited by missing `scikit-misc`.\n",
    "\n",
    "## 5.2 Model Performance\n",
    "\n",
    "- **RF Advantage**: Non-linear modeling outperforms logistic linearity, validated by AUC gains.  \n",
    "- **Threshold (0.06)**: Driven by high FN cost, ensuring maximal recall (1.0) at the expense of lower precision.\n",
    "\n",
    "## 5.3 Industry Insights\n",
    "\n",
    "- **Manufacturing** Edge: Higher AUC/F1 suggest more structured, consistent predictors (e.g., labor costs).  \n",
    "- **Services** Noise: Lower precision due to sector diversity.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Conclusion\n",
    "\n",
    "RF with a `0.06` threshold is recommended for deployment, offering `AUC=0.66-0.69`, `RMSE=0.43`, and `expected loss=0.68` across tasks. Future work could enhance features or adjust cost ratios.\n",
    "\n",
    "Full code is available at [GitHub: DA3AS2](https://github.com/Karoly97/DA3AS2/tree/main).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96535e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
